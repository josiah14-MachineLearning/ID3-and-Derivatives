import coconut.convenience
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
from pandas.core.groupby.generic import DataFrameGroupBy
from typing import List, Tuple, Dict, Any


def entropy(
    total_records: int,
    value_frequencies: np.array,
    log_base: int = 2
) -> float =
    item_probs = value_frequencies / total_records
    -(item_probs * np.log(item_probs) / np.log(log_base)).sum()


def frame_entropy(df: DataFrame, target_feature: str) -> float =
    grouped_df = df.groupby(target_feature)
    counts = map(
        k -> len(grouped_df.get_group(k).index),
        grouped_df.indices.keys()
    )
    counts |> list |> np.array |> entropy$(len(df.index))


def remaining_entropy(
    original_df: DataFrame,
    target_feature: str,
    grouped_df: DataFrameGroupBy
) -> float =
    def weighted_group_entropy(df: DataFrame) -> float = (
        len(df.index)
        / len(original_df.index)
        * frame_entropy(df, target_feature)
    )
    grouped_frames = map(grouped_df.get_group, grouped_df.indices.keys())
    grouped_frames |> map$(weighted_group_entropy) |> list |> np.array |> .sum()


def information_gain(
    target_feature: str,
    original_entropy: float,
    original_df: DataFrame,
    grouped_df: DataFrameGroupBy
) -> float =
    original_entropy - remaining_entropy(original_df, target_feature, grouped_df)


# Change this function to return a data object.
def find_most_informative_feature(
    target_feature: str,
    df: DataFrame
) -> (str, float, DataFrameGroupBy) =
    original_entropy = frame_entropy(df, target_feature)
    def calc_IG(descriptive_feature: str) -> (str, float, DataFrameGroupBy) =
        grouped_df = df.groupby(descriptive_feature)
        (
            descriptive_feature,
            information_gain(target_feature, original_entropy, df, grouped_df),
            grouped_df
        )

    def keep_greatest_information_gain(
        acc_df: (float, str, DataFrameGroupBy),
        next_descriptive_feature: str
    ) -> (str, float, DataFrameGroupBy) =
        next_df = calc_IG(next_descriptive_feature)
        acc_df if acc_df[1] >= next_df[1] else next_df

    descriptive_features = df.drop(target_feature, axis=1).columns |> list
    descriptive_features[0] = calc_IG(descriptive_features[0])

    # If there's only 1 descriptive feature, reduce will just return the only
    # list item, so we don't need an if-statement to provide the shortcut
    # explicitly mentioned in textbook and online explanations of the ID3
    # algorithm.
    reduce(keep_greatest_information_gain, descriptive_features)


def id3(target_feature: str, df: DataFrame) -> Tuple[str, Dict[Any, Any]] =
    """
    High-level algorithm summary:

    1. If all of the target_feature values in the training set DataFrame are the
       same value, return that value as the new leaf.
    2. If there is only one descriptive_feature left to split on, split by it and
       make a new node which is a tuple where the first tuple value is the name
       of the descriptive_feature column, and the second is a dictionary where
       the keys are each unique value of the descriptive_feature column, and the
       values are the mode of the target_feature of that grouping of the
       descriptive_feature value.  Also add an `otherwise` key whose value is
       the mode of the unsplit frame's target_feature column in case real-world
       data contains unique values of the descriptive feature that got excluded
       via former iterations of splitting the training set to build this model.
    3. Otherwise, identify the descriptive_feature which yields the greatest
       Information Gain and use it to split the training set DataFrame.  Make
       a new node which is a tuple where the first tuple value is the name of
       the descriptive_feature column, and the second is the a dictionary where
       the keys are each unique value of the descriptive_feature column, and
       the values are the result of running this id3 function recursively over
       the DataFrame for the group at the key (which is the descriptive_feature
       value).  As in step 2, also add an `otherwise` key whose value is
       the mode of the unsplit frame's target_feature column in case real-world
       data contains unique values of the descriptive feature that got excluded
       via former iterations of splitting the training set to build this model.

    :param target_feature: The name of the column this model should try to
        predict.
    :param df: The training set to use to build this decision tree model.

    :return: A tuple which is a decision tree structure.  To explain the
        mypy type signature, the Dict's first Any represents the type of
        the descriptive_feature at that node, and the second Any could be either
        another tree node, which would have signature Tuple[str, Dict[Any, Any]],
        or it could be a leaf, which would just be a str value which is the value
        predicted at the end of that traversal of the tree.

        Here are some sample trees:
        # simple Ham or Spam prediction example
        ("SuspiciousWords", {True: 'spam', False: 'ham'})

        # sample Ecological Vegetation decision tree
        ("Elevation", {
            "low": "riparian",
            "highest": "conifer",
            "medium": ("Stream, {
                True: "riparian",
                False: "chaparal"
            }),
            "high": ("Slope", {
                "flat": "conifer",
                "steep": "chaparal",
                # below, "moderate" was eliminated through splitting, so the
                # mode of the target_level of the frame unsplit by "Slope" is
                # assumed via "otherwise" for any value other than "flat" or
                # "steep", which covers "moderate" since it got excluded by
                # the split.
                "otherwise": "chaparal" 
            })
        })
    """
    unique_target_values = df[target_feature].unique()
    if len(unique_target_values) == 1:
        new_node = unique_target_values[0]  # a leaf
    else:  # Can skip the rest if the previous if conditional was True
        # Each node will have the column name, the mode at this level, and a list
        # of tuples where the first tuple value is a column value and the second
        # is the next Frame to run id3 against.
        best_feature = find_most_informative_feature(target_feature, df)
        print(best_feature) # for debugging purposes...
        grouped_df: DataFrameGroupBy = best_feature[-1]
        # The default 'otherwise' is done in case unique values of the descriptive feature that
        # got excluded through splitting the training set turn up in the real data.  In
        # this case, the current unsplit mode of the target_feature is assumed to be the most
        # accurate prediction.
        new_leaves = {'otherwise': df[target_feature].mode()}
        new_leaves.update({
            key: id3(target_feature, grouped_df.get_group(key))
            for key in grouped_df.indices.keys()
        })
        new_node = (best_feature[0], new_leaves)
    new_node


def run_dtree_model(dtree: Tuple[str, Dict[Any, Any]], row: Series) -> Series =
    """
    if there is a 50/50 even split on the target feature and this leads to a leaf, both
    (or all in the case of a 3-way 33.33/33.33/33.33 split, and so on for 4, 5, etc
    prediction values) will be returned. No matter the case, the predictions are always
    returned as a pandas.Series, so you'll have a column of Series objects after applying
    this model **NOTE** Except that during testing, it turns out
    that Pandas knows how to infer that the value inside the Series
    instance should be unwrapped into its raw type (such as int or str)
    """
    if isinstance(dtree, tuple):
        feature, branch_dict = dtree
        prediction = (
            branch_dict[row[feature]] 
            if feature in row 
            else branch_dict['otherwise']
        )
        return_val = (
            run_dtree_model(prediction, row) 
            if isinstance(prediction, tuple) 
            else row.append(Series([prediction]))
        )
    else:
        return_val = row.append(Series([dtree]))
    return_val


################################################################################
# Below, I prove that the model predicts back the same target values for each
# row of the training datasets.
################################################################################

spam_analysis_data = {
    'SpamId': [376,489,541,693,782,976],
    'SuspiciousWords': [True,True,True,False,False,False],
    'UnknownSender': [False,True,True,True,False,False],
    'Images': [True,False,False,True,False,False],
    'SpamClass': ["spam","spam","spam","ham","ham","ham"]
}


ecological_vegetation_data = {
    'Id': [1,2,3,4,5,6,7],
    'Stream': [False, True, True, False, False, True, True],
    'Slope': ['steep', 'moderate', 'steep', 'steep', 'flat', 'steep', 'steep'],
    'Elevation': ['high', 'low', 'medium', 'medium', 'high', 'highest', 'high'],
    'Vegetation': ['chaparal', 'riparian', 'riparian', 'chaparal', 'conifer', 'conifer', 'chaparal']
}

spam_analysis_df = pd.DataFrame(spam_analysis_data).drop('SpamId', axis=1)
spam_prediction_model = spam_analysis_df |> id3$('SpamClass')
print()
spam_prediction_model |> print
print()
run_dtree_model$(spam_prediction_model) |> spam_analysis_df.apply$(axis=1) |> print

print()
print('----------------------------------------------------------------------')
print()

ecological_vegetation_df = pd.DataFrame(ecological_vegetation_data).drop('Id', axis=1)
eco_veg_model = ecological_vegetation_df |> id3$('Vegetation')
print()
eco_veg_model |> print
print()
run_dtree_model$(eco_veg_model) |> ecological_vegetation_df.apply$(axis=1) |> print

print()
print('----------------------------------------------------------------------')
print()
# The following dataset comes from https://archive.ics.uci.edu/ml/datasets/Acute+Inflammations
acute_inflammations_df = pd.read_csv(
    '../../../datasets/acute_diagnoses/diagnosis.data',
    sep='\t',
    lineterminator='\n',
    header=None,
    encoding='utf-8'
)
acute_inflammations_df[7] = acute_inflammations_df[7].str.strip('\r')
acute_inflammations_df = acute_inflammations_df.rename(columns={
    0: 'Temperature',
    1: 'Nausea',
    2: 'LumbarPain',
    3: 'UrinePushing',
    4: 'MicturationPains',
    5: 'UrethraBurning',
    6: 'BladderInflammation',
    7: 'RenalPelvisNephritis'
})
acute_inflammations_id3_df = acute_inflammations_df.drop('Temperature', axis=1)
acute_inflammations_id3_predict_bladder_inflammation_df = (
    acute_inflammations_id3_df.drop('RenalPelvisNephritis', axis=1)
)

bladder_inflammations_model = (
    acute_inflammations_id3_predict_bladder_inflammation_df 
    |> id3$('BladderInflammation') 
)
print()
bladder_inflammations_model |> print
print()
(
    run_dtree_model$(bladder_inflammations_model) 
    |> acute_inflammations_id3_predict_bladder_inflammation_df.apply$(axis=1)
    |> print
)

print()
print('----------------------------------------------------------------------')
print()

acute_inflammations_id3_predict_nephritis_df = acute_inflammations_id3_df.drop(
    'BladderInflammation', axis=1
)
nephritis_model = (
    acute_inflammations_id3_predict_nephritis_df 
    |> id3$('RenalPelvisNephritis') 
)
print()
nephritis_model |> print
print()
(
    run_dtree_model$(nephritis_model) 
    |> acute_inflammations_id3_predict_nephritis_df.apply$(axis=1)
    |> print
)
